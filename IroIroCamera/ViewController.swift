//
//  ViewController.swift
//  IroIroCamera
//
//  Created by Hiroki Taniguchi on 2017/07/15.
//  Copyright Â© 2017å¹´ Hiroki Taniguchi. All rights reserved.
//

import UIKit
import AVFoundation
import CoreImage
import StoreKit
import Spring


class ViewController: UIViewController, AVCaptureMetadataOutputObjectsDelegate, AVCaptureVideoDataOutputSampleBufferDelegate {

    @IBOutlet weak var previewView: UIView!
    @IBOutlet weak var takedPicturePreview: UIButton!
    
    let captureSession = AVCaptureSession()
    var videoLayer: AVCaptureVideoPreviewLayer?
    var videoOutput = AVCaptureVideoDataOutput()
    var markFirstView: UIView!
    var markSecondView: UIView!
    var outputImage: UIImage?
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        self.makeMarksView()
        self.setCameraView()
        
    }
    
    func setCameraView() {
        // å…¥åŠ›ï¼ˆèƒŒæ™¯ã‚«ãƒ¡ãƒ©ï¼‰
        let videoDevice = AVCaptureDevice.default(for: AVMediaType.video)
        let videoInput = try! AVCaptureDeviceInput.init(device: videoDevice!)
        captureSession.addInput(videoInput)
        
        // å‡ºåŠ› (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)
        //        let metadataOutput = AVCaptureMetadataOutput()
        //        captureSession.addOutput(metadataOutput)
        
        // QRã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã—ãŸéš›ã®ãƒ‡ãƒªã‚²ãƒ¼ãƒˆè¨­å®š
        //        metadataOutput.setMetadataObjectsDelegate(self, queue: DispatchQueue.main)
        //        metadataOutput.metadataObjectTypes = [AVMetadataObjectTypeQRCode]
        
        //        videoLayer = AVCaptureVideoPreviewLayer.init(session: captureSession)
        //        videoLayer?.frame = previewView.bounds
        //        videoLayer?.videoGravity = AVLayerVideoGravityResizeAspectFill
        //        previewView.layer.addSublayer(videoLayer!)
        
        videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as AnyHashable as! String : Int(kCVPixelFormatType_32BGRA)]
        
        //ãƒ•ãƒ¬ãƒ¼ãƒ æ¯ã«å‘¼ã³å‡ºã™ãƒ‡ãƒªã‚²ãƒ¼ãƒˆç™»éŒ²
        //let queue:DispatchQueue = DispatchQueue(label:"myqueue",attribite: DISPATCH_QUEUE_SERIAL)
        let queue:DispatchQueue = DispatchQueue(label: "myqueue", attributes: .concurrent)
        videoOutput.setSampleBufferDelegate(self, queue: queue)
        videoOutput.alwaysDiscardsLateVideoFrames = true
        
        captureSession.addOutput(self.videoOutput)
        
        let videoLayer = AVCaptureVideoPreviewLayer(session: captureSession)
        videoLayer.frame = self.view.bounds
        videoLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill
        
        previewView.layer.addSublayer(videoLayer)
        
        //ã‚«ãƒ¡ãƒ©å‘ã
        for connection in self.videoOutput.connections {
            if let conn = connection as? AVCaptureConnection {
                if conn.isVideoOrientationSupported {
                    conn.videoOrientation = AVCaptureVideoOrientation.portrait
                }
            }
        }
        
        self.captureSession.startRunning()
        
        //        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®é–‹å§‹
        //        DispatchQueue.global(qos: .userInitiated).async {
        //
        //            self.captureSession.startRunning()
        //        }
    }
    
    func makeMarksView () {
        // MARKã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¯ã™ã‚‹ãƒ“ãƒ¥ãƒ¼
        markFirstView = UIView()
        markFirstView.layer.borderWidth = 4
        markFirstView.frame = CGRect(x: 0, y: 0, width: 0, height: 0)
        markFirstView.layer.cornerRadius = markFirstView.frame.width / 2
        markFirstView.layer.borderColor = UIColor.white.cgColor
        view.addSubview(markFirstView)
    }
    
    func imageFromSampleBuffer(sampleBuffer: CMSampleBuffer) -> UIImage {
        //ãƒãƒƒãƒ•ã‚¡ãƒ¼ã‚’UIImageã«å¤‰æ›
        let imageBuffer: CVImageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)!
        CVPixelBufferLockBaseAddress(imageBuffer, CVPixelBufferLockFlags(rawValue: 0))
        let baseAddress = CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0)
        let bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer)
        let width = CVPixelBufferGetWidth(imageBuffer)
        let height = CVPixelBufferGetHeight(imageBuffer)
        
        let colorSpace = CGColorSpaceCreateDeviceRGB()
        let bitmapInfo = (CGBitmapInfo.byteOrder32Little.rawValue | CGImageAlphaInfo.premultipliedFirst.rawValue)
        let context = CGContext(data: baseAddress, width: width, height: height, bitsPerComponent: 8, bytesPerRow: bytesPerRow, space: colorSpace, bitmapInfo: bitmapInfo)
        let imageRef = context!.makeImage()
        
        CVPixelBufferUnlockBaseAddress(imageBuffer, CVPixelBufferLockFlags(rawValue: 0))
        let resultImage = UIImage(cgImage: imageRef!)

        return resultImage
    }
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)
    {
        //åŒæœŸå‡¦ç†ï¼ˆéåŒæœŸå‡¦ç†ã§ã¯ã‚­ãƒ¥ãƒ¼ãŒæºœã¾ã‚Šã™ãã¦ç”»é¢ãŒã¤ã„ã¦ã„ã‹ãªã„ï¼‰
        DispatchQueue.main.sync(execute: {
            
            //ãƒãƒƒãƒ•ã‚¡ãƒ¼ã‚’UIImageã«å¤‰æ›
            let image = self.imageFromSampleBuffer(sampleBuffer: sampleBuffer)
            self.outputImage = self.imageFromSampleBuffer(sampleBuffer: sampleBuffer)
            let ciimage:CIImage! = CIImage(image: image)
            
            let qrDctector : CIDetector = CIDetector(ofType: CIDetectorTypeQRCode, context: nil, options:[CIDetectorAccuracy: CIDetectorAccuracyHigh] )!
            let qrCode: NSArray = qrDctector.features(in: ciimage) as NSArray
            
            if qrCode.count != 0 {
                var rects = Array<CGRect>()
                for feature in qrCode {
                    // åº§æ¨™å¤‰æ›
                    var qrRect : CGRect = (feature as AnyObject).bounds
                    let widthPer = (self.previewView.bounds.width/image.size.width)
                    let heightPer = (self.previewView.bounds.height/image.size.height)
                    
                    // UIKitã¯å·¦ä¸Šã«åŸç‚¹ãŒã‚ã‚‹ãŒã€CoreImageã¯å·¦ä¸‹ã«åŸç‚¹ãŒã‚ã‚‹ã®ã§æƒãˆã‚‹
                    qrRect.origin.y = image.size.height - qrRect.origin.y - qrRect.size.height
                    //å€ç‡å¤‰æ›
                    qrRect.origin.x = qrRect.origin.x * widthPer
                    qrRect.origin.y = qrRect.origin.y * heightPer
                    qrRect.size.width = qrRect.size.width * widthPer
                    qrRect.size.height = qrRect.size.height * heightPer
                    
                    rects.append(qrRect)
                }
                let firstRect = rects[0]
                    markFirstView.layer.borderColor = UIColor.green.cgColor
                    markFirstView.frame = firstRect
            }
            
            
            //CIDetectorAccuracyHighã ã¨é«˜ç²¾åº¦ï¼ˆä½¿ã£ãŸæ„Ÿã˜ã¯é è·é›¢ã«ã‚ˆã‚‹åˆ¤å®šã®ç²¾åº¦ï¼‰ã ãŒå‡¦ç†ãŒé…ããªã‚‹
//            let detector : CIDetector = CIDetector(ofType: CIDetectorTypeFace, context: nil, options:[CIDetectorAccuracy: CIDetectorAccuracyLow] )!
            let faceDetector : CIDetector = CIDetector(ofType: CIDetectorTypeFace, context: nil, options:[CIDetectorAccuracy: CIDetectorAccuracyHigh] )!
            let faces : NSArray = faceDetector.features(in: ciimage) as NSArray
            
            if faces.count != 0
            {
                var rects = Array<CGRect>()
                var smileArray = Array<Bool>()
                var _ : CIFaceFeature = CIFaceFeature()
                for feature in faces {
                    
                    // åº§æ¨™å¤‰æ›
                    var faceRect : CGRect = (feature as AnyObject).bounds
                    let widthPer = (self.previewView.bounds.width/image.size.width)
                    let heightPer = (self.previewView.bounds.height/image.size.height)
                    
                    // UIKitã¯å·¦ä¸Šã«åŸç‚¹ãŒã‚ã‚‹ãŒã€CoreImageã¯å·¦ä¸‹ã«åŸç‚¹ãŒã‚ã‚‹ã®ã§æƒãˆã‚‹
                    faceRect.origin.y = image.size.height - faceRect.origin.y - faceRect.size.height
                    //å€ç‡å¤‰æ›
                    faceRect.origin.x = faceRect.origin.x * widthPer
                    faceRect.origin.y = faceRect.origin.y * heightPer
                    faceRect.size.width = faceRect.size.width * widthPer
                    faceRect.size.height = faceRect.size.height * heightPer
                    
                    rects.append(faceRect)
                    
                    var smileStatus = (feature as AnyObject).hasSmile
                    smileArray.append(smileStatus!)
                }
                let firstRect = rects[0]
                if smileArray[0] == true{
                    markFirstView.layer.borderColor = UIColor.yellow.cgColor
                    markFirstView.frame = firstRect
                }else {
                    markFirstView.layer.borderColor = UIColor.purple.cgColor
                    markFirstView.layer.borderWidth = 10.0
                    markFirstView.frame = firstRect
                }
            }
        })
    }
    
    func metadataOutput(captureOutput: AVCaptureMetadataOutput, didOutput metadataObjects: [AVMetadataObject], from connection: AVCaptureConnection) {
        for metadata in metadataObjects as! [AVMetadataMachineReadableCodeObject] {
            if metadata.type == AVMetadataObject.ObjectType.qr {
                // æ¤œå‡ºä½ç½®ã‚’å–å¾—
                let barCode = videoLayer?.transformedMetadataObject(for: metadata) as! AVMetadataMachineReadableCodeObject
                markFirstView!.frame = barCode.bounds
                if let qrUrl = metadata.stringValue{
                    print("qrUrl", qrUrl)
//                    showQrLinkAlert(qrLink: qrUrl)
                    makeQrCode(url: qrUrl)
                }
            } else if metadata.type == AVMetadataObject.ObjectType.face {
                print("é¡”èªè­˜ã§ããŸ")
            }
        }
    }
    
    @IBAction func takePicture(_ sender: Any) {
        print("tap takePicture!!")

        
        takeStillPicture()
    }
    
    func takeStillPicture(){
        if var _:AVCaptureConnection = videoOutput.connection(with: AVMediaType.video){
            
            //é¡”èªè¨¼ã®markFirstImageã‚’è¿½åŠ ã™ã‚‹
            UIGraphicsBeginImageContextWithOptions(previewView.frame.size, false, 0.0)
            
            self.outputImage?.draw(in: previewView.frame)
            let markFirstImage = markFirstView.toImage()
            markFirstImage?.draw(in: markFirstView.frame)
            
            let newImage = UIGraphicsGetImageFromCurrentImageContext()
            UIGraphicsEndImageContext()
            
            
            if newImage != nil {
                UIImageWriteToSavedPhotosAlbum(newImage!, self, nil, nil)
                setTakedPicturePreview(takedImage: newImage!)
            }else {
                UIImageWriteToSavedPhotosAlbum(outputImage!, self, nil, nil)
                setTakedPicturePreview(takedImage: outputImage!)
            }
            
        }
    }
    
    func setTakedPicturePreview(takedImage: UIImage) {
//        takedPicturePreview.setImage(takedImage, for: UIControlState.normal)
        takedPicturePreview.setBackgroundImage(takedImage, for: UIControlState.normal)
    }
    
    func showQrLinkAlert(qrLink: String) {
        let alert = UIAlertController(
            title: "QRã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿å–ã£ãŸã‚ˆã‚“ğŸ’",
            message: qrLink,
            preferredStyle: .alert)
        let updateAction = UIAlertAction(title: "é£›ã¶", style: .default) {
            action in
            UIApplication.shared.open(URL(string: qrLink)!)
        }
        alert.addAction(updateAction)
        alert.addAction(UIAlertAction(title: "é£›ã°ãªã„", style: .destructive))
        self.present(alert, animated: true, completion: nil)
    }
    
    func makeQrCode(url: String) {
        // NSString ã‹ã‚‰ NSDataã¸å¤‰æ›
        let data = url.data(using: String.Encoding.utf8)!
        
        // QRã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        // NSDataå‹ã§ãƒ‡ãƒ¼ã‚¿ãƒ¼ã‚’ç”¨æ„
        // inputCorrectionLevelã¯ã€èª¤ã‚Šè¨‚æ­£ãƒ¬ãƒ™ãƒ«
        let qr = CIFilter(name: "CIQRCodeGenerator", withInputParameters: ["inputMessage": data, "inputCorrectionLevel": "H"])!
        
//        let sizeTransform = CGAffineTransformMakeScale(10, 10)
//        let qrImage = qr.outputImage!.imageByApplyingTransform(sizeTransform)
        let qrImage = qr.outputImage!
        let newQrImageView = UIImageView(frame: CGRect(x: 0, y: 0, width: 100, height: 100))
        newQrImageView.image = UIImage(ciImage: qrImage)
        view.addSubview(newQrImageView)
    }

    override func didReceiveMemoryWarning() {
        super.didReceiveMemoryWarning()
        // Dispose of any resources that can be recreated.
    }

    private func showReviewAlert() {
        if #available(iOS 10.3, *) {
            // iOS 10.3ä»¥ä¸Šã®å‡¦ç†
            SKStoreReviewController.requestReview()
        } else if let url = URL(string: "itms-apps://itunes.apple.com/app/id1222454517?action=write-review") {
            // iOS 10.3æœªæº€ã®å‡¦ç†
            showAlertController(url: url)
        }

    }
    
    private func showAlertController(url: URL) {
        let alert = UIAlertController(title: "ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãŠé¡˜ã„",
                                      message: "ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\nãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼",
                                      preferredStyle: .alert)
        self.present(alert, animated: true, completion: nil)
        
        let cancelAction = UIAlertAction(title: "ã‚­ãƒ£ãƒ³ã‚»ãƒ«",
                                         style: .cancel,
                                         handler: nil)
        alert.addAction(cancelAction)
        
        let reviewAction = UIAlertAction(title: "ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹",
                                         style: .default,
                                         handler: {
                                            (action:UIAlertAction!) -> Void in
                                            
                                            
                                            if #available(iOS 10.0, *) {
                                                UIApplication.shared.open(url, options: [:])
                                            }
                                            else {
                                                UIApplication.shared.openURL(url)
                                            }
                                            
        })
        alert.addAction(reviewAction)
    }
}


extension UIView {
    func toImage() -> UIImage? {
        UIGraphicsBeginImageContext(self.frame.size)
        guard let context = UIGraphicsGetCurrentContext() else {
            return nil
        }
        self.layer.render(in: context)
        let image = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        
        return image
    }
}


